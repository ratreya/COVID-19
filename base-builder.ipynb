{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def cleanup(merged):\n",
    "    # Standerdize Country Names\n",
    "    merged['Country'].replace('United Kingdom', 'UK', inplace=True)\n",
    "    merged['Country'].replace('Mainland China', 'China', inplace=True)\n",
    "    merged['Country'].replace(['Korea, South', 'Republic of Korea'], 'South Korea', inplace=True)\n",
    "    merged['Country'].replace('Iran (Islamic Republic of)', 'Iran',inplace=True)\n",
    "\n",
    "    # Standerdize US State Names\n",
    "    merged['State'] = merged['State'].str.strip()\n",
    "    merged['State'].replace(regex={'^.*Virgin Islands.*$': 'Virgin Islands'}, inplace=True)\n",
    "    merged['State'].replace(regex={'^(.+) \\(From Diamond Princess\\)$': r'\\1'}, inplace=True)\n",
    "    merged['State'].replace(regex={'^.*Princess.*$': 'Cruise Ship'}, inplace=True)\n",
    "    merged['State'].replace(regex={'^.+, (.+)$': r'\\1'}, inplace=True)\n",
    "    merged['State'].replace(['District of Columbia', 'D.C.'], 'DC', inplace=True)\n",
    "    merged['State'].replace('Chicago', 'IL', inplace=True)\n",
    "    us_state_abbrev = {\n",
    "        'Alabama': 'AL',\n",
    "        'Alaska': 'AK',\n",
    "        'American Samoa': 'AS',\n",
    "        'Arizona': 'AZ',\n",
    "        'Arkansas': 'AR',\n",
    "        'California': 'CA',\n",
    "        'Colorado': 'CO',\n",
    "        'Connecticut': 'CT',\n",
    "        'Delaware': 'DE',\n",
    "        'District of Columbia': 'DC',\n",
    "        'Florida': 'FL',\n",
    "        'Georgia': 'GA',\n",
    "        'Guam': 'GU',\n",
    "        'Hawaii': 'HI',\n",
    "        'Idaho': 'ID',\n",
    "        'Illinois': 'IL',\n",
    "        'Indiana': 'IN',\n",
    "        'Iowa': 'IA',\n",
    "        'Kansas': 'KS',\n",
    "        'Kentucky': 'KY',\n",
    "        'Louisiana': 'LA',\n",
    "        'Maine': 'ME',\n",
    "        'Maryland': 'MD',\n",
    "        'Massachusetts': 'MA',\n",
    "        'Michigan': 'MI',\n",
    "        'Minnesota': 'MN',\n",
    "        'Mississippi': 'MS',\n",
    "        'Missouri': 'MO',\n",
    "        'Montana': 'MT',\n",
    "        'Nebraska': 'NE',\n",
    "        'Nevada': 'NV',\n",
    "        'New Hampshire': 'NH',\n",
    "        'New Jersey': 'NJ',\n",
    "        'New Mexico': 'NM',\n",
    "        'New York': 'NY',\n",
    "        'North Carolina': 'NC',\n",
    "        'North Dakota': 'ND',\n",
    "        'Northern Mariana Islands':'MP',\n",
    "        'Ohio': 'OH',\n",
    "        'Oklahoma': 'OK',\n",
    "        'Oregon': 'OR',\n",
    "        'Pennsylvania': 'PA',\n",
    "        'Puerto Rico': 'PR',\n",
    "        'Rhode Island': 'RI',\n",
    "        'South Carolina': 'SC',\n",
    "        'South Dakota': 'SD',\n",
    "        'Tennessee': 'TN',\n",
    "        'Texas': 'TX',\n",
    "        'Utah': 'UT',\n",
    "        'Vermont': 'VT',\n",
    "        'Virgin Islands': 'VI',\n",
    "        'Virginia': 'VA',\n",
    "        'Washington': 'WA',\n",
    "        'West Virginia': 'WV',\n",
    "        'Wisconsin': 'WI',\n",
    "        'Wyoming': 'WY'\n",
    "    }\n",
    "    merged['State'].replace(us_state_abbrev, inplace=True)\n",
    "    \n",
    "def fillin(merged):\n",
    "    # Fill NaNs otherwise some operations such as gorupby will not work\n",
    "    merged['Confirmed'].fillna(0, inplace=True)\n",
    "    merged['Deaths'].fillna(0, inplace=True)\n",
    "    merged['Recovered'].fillna(0, inplace=True)\n",
    "    merged['State'].fillna('n/a', inplace=True)\n",
    "    merged['County'].fillna('n/a', inplace=True)\n",
    "    return merged\n",
    "\n",
    "def verify(merged):\n",
    "    # Run verifications - ignore small deviations\n",
    "    df_neg = merged[(merged['County'] != 'Unassigned') & (merged['Confirmed_New'] < -100) | (merged['Deaths_New'] < -50) | (merged['Recovered_New'] < -50)]\n",
    "    if df_neg.shape[0] > 0:\n",
    "        print('Some deltas are hugely negative!')\n",
    "        print(df_neg.sort_values('Confirmed_New'))\n",
    "\n",
    "    mismatch = merged[(merged['State'] != 'US') & (merged['State'] != 'Recovered') & (merged['County'] != 'Unassigned') & (merged['Confirmed'] - (merged['Deaths'] + merged['Recovered']) < -10)]\n",
    "    if mismatch.shape[0] > 0:\n",
    "        print('Confirmed is much smaller than Deaths + Recovered!')\n",
    "        print(mismatch)\n",
    "\n",
    "def jhu():\n",
    "    # Get list of days in expected format\n",
    "    sdate = date(2020, 1, 22)\n",
    "    today = date.today()\n",
    "    edate = date(today.year, today.month, today.day)\n",
    "    days = [(sdate + timedelta(days=i)).strftime('%m-%d-%Y') for i in range((edate - sdate).days + 1)]\n",
    "\n",
    "    # Merge all daily reports\n",
    "    url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/%s.csv'\n",
    "    with ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        futures = [executor.submit(lambda day: (day, pd.read_csv(url % day)), day) for day in days]\n",
    "\n",
    "    merged = pd.DataFrame(columns = ['Country', 'State', 'County', 'Date', 'Confirmed', 'Deaths', 'Recovered', 'Confirmed_New', 'Deaths_New', 'Recovered_New', 'Confirmed_Doubling'])\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            day, df = future.result()\n",
    "            # Fix changes in column names if they exits\n",
    "            df = df.rename({'Admin2': 'County', 'Province/State':'State', 'Country/Region':'Country', 'Province_State':'State', 'Country_Region':'Country'}, axis=1)\n",
    "            df.drop([x for x in df.columns.values if x not in merged.columns.values], axis=1, inplace=True)\n",
    "            df['Date'] = day\n",
    "            merged = pd.concat([merged, df])\n",
    "        except IOError as e:\n",
    "            print(str(e))\n",
    "\n",
    "    # Clean up the data\n",
    "    cleanup(merged)\n",
    "    fillin(merged)\n",
    "\n",
    "    # Fix bad data\n",
    "    merged.loc[(merged['State'] == 'French Polynesia') & (merged['Date'] == '03-23-2020'), 'State'] = 'n/a'\n",
    "    merged.loc[(merged['Country'] == 'France') & (merged['State'] == 'France'), 'State'] = 'n/a'\n",
    "\n",
    "    # Do this because there are duplicate rows in some datasets\n",
    "    return merged.groupby(['Country', 'State', 'County', 'Date'], as_index=False).sum()\n",
    "\n",
    "def india(merged):\n",
    "    df = pd.read_csv('covid_19_india.csv')\n",
    "    df.Date = [datetime.strptime(x, '%d/%m/%y').strftime('%m-%d-%Y') for x in df.Date]\n",
    "    df = df.rename({'State/UnionTerritory': 'State', 'Cured': 'Recovered'}, axis=1)\n",
    "    df.drop([x for x in df.columns.values if x not in merged.columns.values], axis=1, inplace=True)\n",
    "    df['Country'] = 'India'\n",
    "    df['County'] = 'n/a'\n",
    "    return fillin(pd.concat([merged, df]))\n",
    "\n",
    "# Takes a list of numbers and returns a list with number of rows to double\n",
    "def doubling(data):\n",
    "    doubling = []\n",
    "    for i in reversed(range(len(data))):\n",
    "        index = -1\n",
    "        for j in reversed(range(i)):\n",
    "            if data[i] >= data[j]*2:\n",
    "                index = j\n",
    "                break\n",
    "        doubling.append(i - index if index > 0 else np.nan)\n",
    "    return list(reversed(doubling))\n",
    "\n",
    "def deltas(merged):\n",
    "    def deltas(df):\n",
    "        for state in df['State'].unique():\n",
    "            for county in df[df['State'] == state]['County'].unique():\n",
    "                confirmed = df[(df['State'] == state) & (df['County'] == county)]['Confirmed'].values.tolist()\n",
    "                confirmed_deltas = [np.nan] + [confirmed[i] - confirmed[i-1] for i in range(1, len(confirmed))]\n",
    "                df.loc[(df['State'] == state) & (df['County'] == county), 'Confirmed_New'] = confirmed_deltas\n",
    "                deaths = df[(df['State'] == state) & (df['County'] == county)]['Deaths'].values.tolist()\n",
    "                deaths_deltas = [np.nan] + [deaths[i] - deaths[i-1] for i in range(1, len(deaths))]\n",
    "                df.loc[(df['State'] == state) & (df['County'] == county), 'Deaths_New'] = deaths_deltas\n",
    "                recovered = df[(df['State'] == state) & (df['County'] == county)]['Recovered'].values.tolist()\n",
    "                recovered_deltas = [np.nan] + [recovered[i] - recovered[i-1] for i in range(1, len(recovered))]\n",
    "                df.loc[(df['State'] == state) & (df['County'] == county), 'Recovered_New'] = recovered_deltas\n",
    "                df.loc[(df['State'] == state) & (df['County'] == county), 'Confirmed_Doubling'] = doubling(confirmed)\n",
    "        return df\n",
    "\n",
    "    # Calculate deltas for each date\n",
    "    with ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        futures = [executor.submit(deltas, merged[(merged['Country'] == 'US') & (merged['State'] == state)].copy()) for state in merged[merged['Country'] == 'US'].State.unique()]\n",
    "        futures += [executor.submit(deltas, merged[merged['Country'] == country].copy()) for country in merged.Country.unique() if country != 'US']\n",
    "\n",
    "    final = pd.DataFrame(columns=merged.columns)\n",
    "    for future in as_completed(futures):\n",
    "        df = future.result()\n",
    "        final = pd.concat([final, df])\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n",
      "Some deltas are hugely negative!\n",
      "                   Country      State         County        Date  Confirmed  \\\n",
      "8152                France        n/a            n/a  04-14-2020   130253.0   \n",
      "8160                France        n/a            n/a  04-22-2020   155860.0   \n",
      "82140                   US         NJ     Unassigned  04-22-2020     1045.0   \n",
      "84980                   US         NY  New York City  04-23-2020   145855.0   \n",
      "11092                Japan        n/a            n/a  04-28-2020    13736.0   \n",
      "85170                   US         NY         Orange  04-23-2020     6816.0   \n",
      "85322                   US         NY         Putnam  04-23-2020      615.0   \n",
      "100356                  US         TN       Davidson  04-17-2020     1307.0   \n",
      "124342  West Bank and Gaza        n/a            n/a  04-25-2020      342.0   \n",
      "100352                  US         TN       Davidson  04-13-2020     1207.0   \n",
      "95816                   US         PR            n/a  04-24-2020     1276.0   \n",
      "22771                   US         AZ         Navajo  04-09-2020      286.0   \n",
      "81167                   US         NH   Hillsborough  04-14-2020      271.0   \n",
      "75143                   US         NC      Granville  04-28-2020      133.0   \n",
      "11758            Lithuania        n/a            n/a  04-28-2020     1344.0   \n",
      "58039                   US         MA     Unassigned  04-25-2020      753.0   \n",
      "58041                   US         MA     Unassigned  04-27-2020      795.0   \n",
      "2890                Canada  Recovered            n/a  04-01-2020        0.0   \n",
      "85906                   US         NY     Unassigned  04-20-2020        0.0   \n",
      "85905                   US         NY     Unassigned  04-19-2020        0.0   \n",
      "85896                   US         NY     Unassigned  04-10-2020        0.0   \n",
      "85895                   US         NY     Unassigned  04-09-2020        0.0   \n",
      "85893                   US         NY     Unassigned  04-07-2020        0.0   \n",
      "85892                   US         NY     Unassigned  04-06-2020        0.0   \n",
      "116203                  US         VA     Unassigned  04-22-2020        0.0   \n",
      "58947                   US         MD     Unassigned  04-27-2020        0.0   \n",
      "1002             Australia   Victoria            n/a  04-25-2020     1346.0   \n",
      "58029                   US         MA     Unassigned  04-15-2020      572.0   \n",
      "58025                   US         MA     Unassigned  04-11-2020      225.0   \n",
      "4458                 China      Hubei            n/a  04-17-2020    68128.0   \n",
      "13022          Netherlands        n/a            n/a  04-21-2020    34134.0   \n",
      "82119                   US         NJ     Unassigned  04-01-2020     4512.0   \n",
      "16521                   UK        n/a            n/a  04-13-2020    88621.0   \n",
      "\n",
      "         Deaths  Recovered  Confirmed_New  Deaths_New  Recovered_New  \\\n",
      "8152    15729.0    28805.0        -6526.0       762.0         1087.0   \n",
      "8160    21340.0    40657.0        -2190.0       544.0         1476.0   \n",
      "82140       0.0        0.0        -2028.0      -233.0            0.0   \n",
      "84980   16388.0        0.0        -1442.0      1314.0            0.0   \n",
      "11092     394.0     1899.0         -417.0         9.0            0.0   \n",
      "85170     268.0        0.0         -336.0        24.0            0.0   \n",
      "85322       7.0        0.0         -216.0         0.0            0.0   \n",
      "100356     19.0        0.0         -185.0         0.0            0.0   \n",
      "124342      2.0       92.0         -142.0        -2.0            0.0   \n",
      "100352     16.0        0.0         -142.0         3.0            0.0   \n",
      "95816      77.0        0.0         -140.0         8.0            0.0   \n",
      "22771       1.0        0.0         -118.0         0.0            0.0   \n",
      "81167       0.0        0.0         -114.0        -2.0            0.0   \n",
      "75143       5.0        0.0         -107.0         0.0            0.0   \n",
      "11758      44.0      536.0         -105.0         3.0           62.0   \n",
      "58039      13.0        0.0          -38.0      -590.0            0.0   \n",
      "58041       8.0        0.0          -12.0      -133.0            0.0   \n",
      "2890        0.0     1324.0            0.0         0.0         -268.0   \n",
      "85906      48.0        0.0            0.0      -353.0            0.0   \n",
      "85905     401.0        0.0            0.0      -658.0            0.0   \n",
      "85896       0.0        0.0            0.0       -73.0            0.0   \n",
      "85895      73.0        0.0            0.0       -79.0            0.0   \n",
      "85893     116.0        0.0            0.0      -193.0            0.0   \n",
      "85892     309.0        0.0            0.0      -951.0            0.0   \n",
      "116203      0.0        0.0            0.0      -140.0            0.0   \n",
      "58947      87.0        0.0            0.0       -58.0            0.0   \n",
      "1002       16.0      133.0            3.0         0.0        -1039.0   \n",
      "58029      14.0        0.0           66.0       -86.0            0.0   \n",
      "58025      13.0        0.0           96.0      -166.0            0.0   \n",
      "4458     4512.0    63487.0          325.0      1290.0         -948.0   \n",
      "13022    3916.0        0.0          729.0       165.0         -250.0   \n",
      "82119       0.0        0.0          826.0      -247.0            0.0   \n",
      "16521   11329.0        0.0         4342.0       717.0         -344.0   \n",
      "\n",
      "        Confirmed_Doubling  \n",
      "8152                  11.0  \n",
      "8160                  19.0  \n",
      "82140                 30.0  \n",
      "84980                 17.0  \n",
      "11092                 16.0  \n",
      "85170                 18.0  \n",
      "85322                 19.0  \n",
      "100356                17.0  \n",
      "124342                23.0  \n",
      "100352                13.0  \n",
      "95816                 16.0  \n",
      "22771                  9.0  \n",
      "81167                 12.0  \n",
      "75143                 23.0  \n",
      "11758                 26.0  \n",
      "58039                 13.0  \n",
      "58041                 15.0  \n",
      "2890                   1.0  \n",
      "85906                  1.0  \n",
      "85905                  1.0  \n",
      "85896                  1.0  \n",
      "85895                  1.0  \n",
      "85893                  1.0  \n",
      "85892                  1.0  \n",
      "116203                 1.0  \n",
      "58947                  1.0  \n",
      "1002                  29.0  \n",
      "58029                  4.0  \n",
      "58025                 18.0  \n",
      "4458                  65.0  \n",
      "13022                 17.0  \n",
      "82119                  5.0  \n",
      "16521                  9.0  \n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "merged = jhu()\n",
    "merged = india(merged[merged['Country'] != 'India'])\n",
    "merged = deltas(merged)\n",
    "merged = merged.groupby(['Country', 'State', 'County', 'Date'], as_index=False).sum()\n",
    "\n",
    "# Write merged to CSV and verify\n",
    "merged.to_csv('jhu-daily-reports.csv', index=False)\n",
    "verify(merged)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
