{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def cleanup(merged):\n",
    "    # Standerdize Country Names\n",
    "    merged['Country'].replace('United Kingdom', 'UK', inplace=True)\n",
    "    merged['Country'].replace('Mainland China', 'China', inplace=True)\n",
    "    merged['Country'].replace(['Korea, South', 'Republic of Korea'], 'South Korea', inplace=True)\n",
    "    merged['Country'].replace('Iran (Islamic Republic of)', 'Iran',inplace=True)\n",
    "\n",
    "    # Standerdize US State Names\n",
    "    merged['State'] = merged['State'].str.strip()\n",
    "    merged['State'].replace(regex={'^.*Virgin Islands.*$': 'Virgin Islands'}, inplace=True)\n",
    "    merged['State'].replace(regex={'^(.+) \\(From Diamond Princess\\)$': r'\\1'}, inplace=True)\n",
    "    merged['State'].replace(regex={'^.*Princess.*$': 'Cruise Ship'}, inplace=True)\n",
    "    merged['State'].replace(regex={'^.+, (.+)$': r'\\1'}, inplace=True)\n",
    "    merged['State'].replace(['District of Columbia', 'D.C.'], 'DC', inplace=True)\n",
    "    merged['State'].replace('Chicago', 'IL', inplace=True)\n",
    "    us_state_abbrev = {\n",
    "        'Alabama': 'AL',\n",
    "        'Alaska': 'AK',\n",
    "        'American Samoa': 'AS',\n",
    "        'Arizona': 'AZ',\n",
    "        'Arkansas': 'AR',\n",
    "        'California': 'CA',\n",
    "        'Colorado': 'CO',\n",
    "        'Connecticut': 'CT',\n",
    "        'Delaware': 'DE',\n",
    "        'District of Columbia': 'DC',\n",
    "        'Florida': 'FL',\n",
    "        'Georgia': 'GA',\n",
    "        'Guam': 'GU',\n",
    "        'Hawaii': 'HI',\n",
    "        'Idaho': 'ID',\n",
    "        'Illinois': 'IL',\n",
    "        'Indiana': 'IN',\n",
    "        'Iowa': 'IA',\n",
    "        'Kansas': 'KS',\n",
    "        'Kentucky': 'KY',\n",
    "        'Louisiana': 'LA',\n",
    "        'Maine': 'ME',\n",
    "        'Maryland': 'MD',\n",
    "        'Massachusetts': 'MA',\n",
    "        'Michigan': 'MI',\n",
    "        'Minnesota': 'MN',\n",
    "        'Mississippi': 'MS',\n",
    "        'Missouri': 'MO',\n",
    "        'Montana': 'MT',\n",
    "        'Nebraska': 'NE',\n",
    "        'Nevada': 'NV',\n",
    "        'New Hampshire': 'NH',\n",
    "        'New Jersey': 'NJ',\n",
    "        'New Mexico': 'NM',\n",
    "        'New York': 'NY',\n",
    "        'North Carolina': 'NC',\n",
    "        'North Dakota': 'ND',\n",
    "        'Northern Mariana Islands':'MP',\n",
    "        'Ohio': 'OH',\n",
    "        'Oklahoma': 'OK',\n",
    "        'Oregon': 'OR',\n",
    "        'Pennsylvania': 'PA',\n",
    "        'Puerto Rico': 'PR',\n",
    "        'Rhode Island': 'RI',\n",
    "        'South Carolina': 'SC',\n",
    "        'South Dakota': 'SD',\n",
    "        'Tennessee': 'TN',\n",
    "        'Texas': 'TX',\n",
    "        'Utah': 'UT',\n",
    "        'Vermont': 'VT',\n",
    "        'Virgin Islands': 'VI',\n",
    "        'Virginia': 'VA',\n",
    "        'Washington': 'WA',\n",
    "        'West Virginia': 'WV',\n",
    "        'Wisconsin': 'WI',\n",
    "        'Wyoming': 'WY'\n",
    "    }\n",
    "    merged['State'].replace(us_state_abbrev, inplace=True)\n",
    "    \n",
    "def fillin(merged):\n",
    "    # Fill NaNs otherwise some operations such as gorupby will not work\n",
    "    merged['Confirmed'].fillna(0, inplace=True)\n",
    "    merged['Deaths'].fillna(0, inplace=True)\n",
    "    merged['Recovered'].fillna(0, inplace=True)\n",
    "    merged['State'].fillna('n/a', inplace=True)\n",
    "    merged['County'].fillna('n/a', inplace=True)\n",
    "    return merged\n",
    "\n",
    "def verify(merged):\n",
    "    # Run verifications - ignore small deviations\n",
    "    df_neg = merged[(merged['County'] != 'Unassigned') & (merged['Confirmed_New'] < -100) | (merged['Deaths_New'] < -50) | (merged['Recovered_New'] < -50)]\n",
    "    if df_neg.shape[0] > 0:\n",
    "        print('Some deltas are hugely negative!')\n",
    "        print(df_neg.sort_values('Confirmed_New'))\n",
    "\n",
    "    mismatch = merged[(merged['State'] != 'US') & (merged['State'] != 'Recovered') & (merged['County'] != 'Unassigned') & (merged['Confirmed'] - (merged['Deaths'] + merged['Recovered']) < -10)]\n",
    "    if mismatch.shape[0] > 0:\n",
    "        print('Confirmed is much smaller than Deaths + Recovered!')\n",
    "        print(mismatch)\n",
    "\n",
    "def jhu():\n",
    "    # Get list of days in expected format\n",
    "    sdate = date(2020, 1, 22)\n",
    "    today = date.today()\n",
    "    edate = date(today.year, today.month, today.day)\n",
    "    days = [(sdate + timedelta(days=i)).strftime('%m-%d-%Y') for i in range((edate - sdate).days + 1)]\n",
    "\n",
    "    # Merge all daily reports\n",
    "    url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/%s.csv'\n",
    "    with ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        futures = [executor.submit(lambda day: (day, pd.read_csv(url % day)), day) for day in days]\n",
    "\n",
    "    merged = pd.DataFrame(columns = ['Country', 'State', 'County', 'Date', 'Confirmed', 'Deaths', 'Recovered', 'Confirmed_New', 'Deaths_New', 'Recovered_New', 'Confirmed_Doubling'])\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            day, df = future.result()\n",
    "            # Fix changes in column names if they exits\n",
    "            df = df.rename({'Admin2': 'County', 'Province/State':'State', 'Country/Region':'Country', 'Province_State':'State', 'Country_Region':'Country'}, axis=1)\n",
    "            df.drop([x for x in df.columns.values if x not in merged.columns.values], axis=1, inplace=True)\n",
    "            df['Date'] = day\n",
    "            merged = pd.concat([merged, df])\n",
    "        except IOError as e:\n",
    "            print(str(e))\n",
    "\n",
    "    # Clean up the data\n",
    "    cleanup(merged)\n",
    "    fillin(merged)\n",
    "\n",
    "    # Fix bad data\n",
    "    merged.loc[(merged['State'] == 'French Polynesia') & (merged['Date'] == '03-23-2020'), 'State'] = 'n/a'\n",
    "    merged.loc[(merged['Country'] == 'France') & (merged['State'] == 'France'), 'State'] = 'n/a'\n",
    "\n",
    "    # Do this because there are duplicate rows in some datasets\n",
    "    return merged.groupby(['Country', 'State', 'County', 'Date'], as_index=False).sum()\n",
    "\n",
    "def india(merged):\n",
    "#     kg.api.authenticate()\n",
    "#     kg.api.dataset_download_file('sudalairajkumar/covid19-in-india', 'covid_19_india.csv', path='./', force=True)\n",
    "    df = pd.read_csv('covid_19_india.csv')\n",
    "    df.Date = [datetime.strptime(x, '%d/%m/%y').strftime('%m-%d-%Y') for x in df.Date]\n",
    "    df = df.rename({'State/UnionTerritory': 'State', 'Cured': 'Recovered'}, axis=1)\n",
    "    df.drop([x for x in df.columns.values if x not in merged.columns.values], axis=1, inplace=True)\n",
    "    df['Country'] = 'India'\n",
    "    df['County'] = 'n/a'\n",
    "    return fillin(pd.concat([merged, df]))\n",
    "\n",
    "def deltas(merged):\n",
    "    def deltas(df):\n",
    "        for state in df['State'].unique():\n",
    "            for county in df[df['State'] == state]['County'].unique():\n",
    "                confirmed = df[(df['State'] == state) & (df['County'] == county)]['Confirmed'].values.tolist()\n",
    "                confirmed_deltas = [np.nan] + [confirmed[i] - confirmed[i-1] for i in range(1, len(confirmed))]\n",
    "                df.loc[(df['State'] == state) & (df['County'] == county), 'Confirmed_New'] = confirmed_deltas\n",
    "                deaths = df[(df['State'] == state) & (df['County'] == county)]['Deaths'].values.tolist()\n",
    "                deaths_deltas = [np.nan] + [deaths[i] - deaths[i-1] for i in range(1, len(deaths))]\n",
    "                df.loc[(df['State'] == state) & (df['County'] == county), 'Deaths_New'] = deaths_deltas\n",
    "                recovered = df[(df['State'] == state) & (df['County'] == county)]['Recovered'].values.tolist()\n",
    "                recovered_deltas = [np.nan] + [recovered[i] - recovered[i-1] for i in range(1, len(recovered))]\n",
    "                df.loc[(df['State'] == state) & (df['County'] == county), 'Recovered_New'] = recovered_deltas\n",
    "        return df\n",
    "\n",
    "    # Calculate deltas for each date\n",
    "    with ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        futures = [executor.submit(deltas, merged[(merged['Country'] == 'US') & (merged['State'] == state)].copy()) for state in merged[merged['Country'] == 'US'].State.unique()]\n",
    "        futures += [executor.submit(deltas, merged[merged['Country'] == country].copy()) for country in merged.Country.unique() if country != 'US']\n",
    "\n",
    "    final = pd.DataFrame(columns=merged.columns)\n",
    "    for future in as_completed(futures):\n",
    "        df = future.result()\n",
    "        final = pd.concat([final, df])\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n",
      "Some deltas are hugely negative!\n",
      "           Country          State         County        Date  Confirmed  \\\n",
      "53684           UK        Unknown            n/a  07-02-2020        0.0   \n",
      "49804        Spain            n/a            n/a  04-30-2020   213435.0   \n",
      "21128       France            n/a            n/a  04-14-2020   130253.0   \n",
      "53692           UK        Unknown            n/a  07-10-2020        0.0   \n",
      "21179       France            n/a            n/a  06-04-2020   185986.0   \n",
      "...            ...            ...            ...         ...        ...   \n",
      "266199          US             NY  New York City  05-08-2020   181783.0   \n",
      "33005   Kazakhstan            n/a            n/a  07-10-2020    56455.0   \n",
      "9094         Chile  Metropolitana            n/a  06-02-2020    88194.0   \n",
      "4747        Brazil           Para            n/a  06-19-2020    80072.0   \n",
      "53893           UK            n/a            n/a  04-13-2020    88621.0   \n",
      "\n",
      "         Deaths  Recovered  Confirmed_New  Deaths_New  Recovered_New  \n",
      "53684       0.0        0.0      -116237.0         0.0            0.0  \n",
      "49804   24543.0   112050.0       -23464.0       268.0       -20879.0  \n",
      "21128   15729.0    28805.0        -6526.0       762.0         1087.0  \n",
      "53692       0.0        0.0        -3338.0         0.0            0.0  \n",
      "21179   29010.0    67510.0        -2850.0        43.0          479.0  \n",
      "...         ...        ...            ...         ...            ...  \n",
      "266199  19561.0        0.0         1567.0       -65.0            0.0  \n",
      "33005     264.0    32500.0         1708.0         0.0        -2637.0  \n",
      "9094        0.0        0.0         2955.0      -824.0            0.0  \n",
      "4747     4469.0    66305.0         3449.0        74.0         -613.0  \n",
      "53893   11329.0        0.0         4342.0       717.0         -344.0  \n",
      "\n",
      "[183 rows x 10 columns]\n",
      "Confirmed is much smaller than Deaths + Recovered!\n",
      "      Country            State County        Date  Confirmed  Deaths  \\\n",
      "4960   Brazil            Piaui    n/a  06-20-2020    12567.0   459.0   \n",
      "4965   Brazil            Piaui    n/a  06-25-2020    17080.0   546.0   \n",
      "4970   Brazil            Piaui    n/a  06-30-2020    20422.0   663.0   \n",
      "4971   Brazil            Piaui    n/a  07-01-2020    22059.0   686.0   \n",
      "4973   Brazil            Piaui    n/a  07-03-2020    24376.0   697.0   \n",
      "...       ...              ...    ...         ...        ...     ...   \n",
      "49666   Spain          Unknown    n/a  05-24-2020        0.0     0.0   \n",
      "49667   Spain          Unknown    n/a  05-25-2020        0.0     0.0   \n",
      "52959      UK  Channel Islands    n/a  05-31-2020      560.0    45.0   \n",
      "52960      UK  Channel Islands    n/a  06-01-2020      560.0    45.0   \n",
      "52961      UK  Channel Islands    n/a  06-02-2020      560.0    46.0   \n",
      "\n",
      "       Recovered  Confirmed_New  Deaths_New  Recovered_New  \n",
      "4960     13029.0            0.0        24.0        12295.0  \n",
      "4965     16570.0          853.0        12.0          895.0  \n",
      "4970     20479.0          669.0        20.0         1601.0  \n",
      "4971     21717.0         1637.0        23.0         1238.0  \n",
      "4973     23880.0         1069.0         0.0         2163.0  \n",
      "...          ...            ...         ...            ...  \n",
      "49666   150376.0            0.0         0.0            0.0  \n",
      "49667   150376.0            0.0         0.0            0.0  \n",
      "52959      528.0            0.0         0.0            3.0  \n",
      "52960      528.0            0.0         0.0            0.0  \n",
      "52961      528.0            0.0         1.0            0.0  \n",
      "\n",
      "[425 rows x 10 columns]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "merged = jhu()\n",
    "merged = india(merged[merged['Country'] != 'India'])\n",
    "merged = deltas(merged)\n",
    "merged = merged.groupby(['Country', 'State', 'County', 'Date'], as_index=False).sum()\n",
    "\n",
    "# Write merged to CSV and verify\n",
    "merged.to_csv('jhu-daily-reports.csv', index=False)\n",
    "verify(merged)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
