{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def cleanup(merged):\n",
    "    # Standerdize Country Names\n",
    "    merged['Country'].replace('United Kingdom', 'UK', inplace=True)\n",
    "    merged['Country'].replace('Mainland China', 'China', inplace=True)\n",
    "    merged['Country'].replace(['Korea, South', 'Republic of Korea'], 'South Korea', inplace=True)\n",
    "    merged['Country'].replace('Iran (Islamic Republic of)', 'Iran',inplace=True)\n",
    "\n",
    "    # Standerdize US State Names\n",
    "    merged['State'] = merged['State'].str.strip()\n",
    "    merged['State'].replace(regex={'^.*Virgin Islands.*$': 'Virgin Islands'}, inplace=True)\n",
    "    merged['State'].replace(regex={'^(.+) \\(From Diamond Princess\\)$': r'\\1'}, inplace=True)\n",
    "    merged['State'].replace(regex={'^.*Princess.*$': 'Cruise Ship'}, inplace=True)\n",
    "    merged['State'].replace(regex={'^.+, (.+)$': r'\\1'}, inplace=True)\n",
    "    merged['State'].replace(['District of Columbia', 'D.C.'], 'DC', inplace=True)\n",
    "    merged['State'].replace('Chicago', 'IL', inplace=True)\n",
    "    us_state_abbrev = {\n",
    "        'Alabama': 'AL',\n",
    "        'Alaska': 'AK',\n",
    "        'American Samoa': 'AS',\n",
    "        'Arizona': 'AZ',\n",
    "        'Arkansas': 'AR',\n",
    "        'California': 'CA',\n",
    "        'Colorado': 'CO',\n",
    "        'Connecticut': 'CT',\n",
    "        'Delaware': 'DE',\n",
    "        'District of Columbia': 'DC',\n",
    "        'Florida': 'FL',\n",
    "        'Georgia': 'GA',\n",
    "        'Guam': 'GU',\n",
    "        'Hawaii': 'HI',\n",
    "        'Idaho': 'ID',\n",
    "        'Illinois': 'IL',\n",
    "        'Indiana': 'IN',\n",
    "        'Iowa': 'IA',\n",
    "        'Kansas': 'KS',\n",
    "        'Kentucky': 'KY',\n",
    "        'Louisiana': 'LA',\n",
    "        'Maine': 'ME',\n",
    "        'Maryland': 'MD',\n",
    "        'Massachusetts': 'MA',\n",
    "        'Michigan': 'MI',\n",
    "        'Minnesota': 'MN',\n",
    "        'Mississippi': 'MS',\n",
    "        'Missouri': 'MO',\n",
    "        'Montana': 'MT',\n",
    "        'Nebraska': 'NE',\n",
    "        'Nevada': 'NV',\n",
    "        'New Hampshire': 'NH',\n",
    "        'New Jersey': 'NJ',\n",
    "        'New Mexico': 'NM',\n",
    "        'New York': 'NY',\n",
    "        'North Carolina': 'NC',\n",
    "        'North Dakota': 'ND',\n",
    "        'Northern Mariana Islands':'MP',\n",
    "        'Ohio': 'OH',\n",
    "        'Oklahoma': 'OK',\n",
    "        'Oregon': 'OR',\n",
    "        'Pennsylvania': 'PA',\n",
    "        'Puerto Rico': 'PR',\n",
    "        'Rhode Island': 'RI',\n",
    "        'South Carolina': 'SC',\n",
    "        'South Dakota': 'SD',\n",
    "        'Tennessee': 'TN',\n",
    "        'Texas': 'TX',\n",
    "        'Utah': 'UT',\n",
    "        'Vermont': 'VT',\n",
    "        'Virgin Islands': 'VI',\n",
    "        'Virginia': 'VA',\n",
    "        'Washington': 'WA',\n",
    "        'West Virginia': 'WV',\n",
    "        'Wisconsin': 'WI',\n",
    "        'Wyoming': 'WY'\n",
    "    }\n",
    "    merged['State'].replace(us_state_abbrev, inplace=True)\n",
    "    \n",
    "def fillin(merged):\n",
    "    # Fill NaNs otherwise some operations such as gorupby will not work\n",
    "    merged['Confirmed'].fillna(0, inplace=True)\n",
    "    merged['Deaths'].fillna(0, inplace=True)\n",
    "    merged['Recovered'].fillna(0, inplace=True)\n",
    "    merged['State'].fillna('n/a', inplace=True)\n",
    "    merged['County'].fillna('n/a', inplace=True)\n",
    "    return merged\n",
    "\n",
    "def verify(merged):\n",
    "    # Run verifications - ignore small deviations\n",
    "    df_neg = merged[(merged['County'] != 'Unassigned') & (merged['Confirmed_New'] < -100) | (merged['Deaths_New'] < -50) | (merged['Recovered_New'] < -50)]\n",
    "    if df_neg.shape[0] > 0:\n",
    "        print('Some deltas are hugely negative!')\n",
    "        print(df_neg.sort_values('Confirmed_New'))\n",
    "\n",
    "    mismatch = merged[(merged['State'] != 'US') & (merged['State'] != 'Recovered') & (merged['County'] != 'Unassigned') & (merged['Confirmed'] - (merged['Deaths'] + merged['Recovered']) < -10)]\n",
    "    if mismatch.shape[0] > 0:\n",
    "        print('Confirmed is much smaller than Deaths + Recovered!')\n",
    "        print(mismatch)\n",
    "\n",
    "def jhu():\n",
    "    # Get list of days in expected format\n",
    "    sdate = date(2020, 1, 22)\n",
    "    today = date.today()\n",
    "    edate = date(today.year, today.month, today.day)\n",
    "    days = [(sdate + timedelta(days=i)).strftime('%m-%d-%Y') for i in range((edate - sdate).days + 1)]\n",
    "\n",
    "    # Merge all daily reports\n",
    "    url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/%s.csv'\n",
    "    with ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        futures = [executor.submit(lambda day: (day, pd.read_csv(url % day)), day) for day in days]\n",
    "\n",
    "    merged = pd.DataFrame(columns = ['Country', 'State', 'County', 'Date', 'Confirmed', 'Deaths', 'Recovered', 'Confirmed_New', 'Deaths_New', 'Recovered_New', 'Confirmed_Doubling'])\n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            day, df = future.result()\n",
    "            # Fix changes in column names if they exits\n",
    "            df = df.rename({'Admin2': 'County', 'Province/State':'State', 'Country/Region':'Country', 'Province_State':'State', 'Country_Region':'Country'}, axis=1)\n",
    "            df.drop([x for x in df.columns.values if x not in merged.columns.values], axis=1, inplace=True)\n",
    "            df['Date'] = day\n",
    "            merged = pd.concat([merged, df])\n",
    "        except IOError as e:\n",
    "            print(str(e))\n",
    "\n",
    "    # Clean up the data\n",
    "    cleanup(merged)\n",
    "    fillin(merged)\n",
    "\n",
    "    # Fix bad data\n",
    "    merged.loc[(merged['State'] == 'French Polynesia') & (merged['Date'] == '03-23-2020'), 'State'] = 'n/a'\n",
    "    merged.loc[(merged['Country'] == 'France') & (merged['State'] == 'France'), 'State'] = 'n/a'\n",
    "\n",
    "    # Do this because there are duplicate rows in some datasets\n",
    "    return merged.groupby(['Country', 'State', 'County', 'Date'], as_index=False).sum()\n",
    "\n",
    "def india(merged):\n",
    "#     kg.api.authenticate()\n",
    "#     kg.api.dataset_download_file('sudalairajkumar/covid19-in-india', 'covid_19_india.csv', path='./', force=True)\n",
    "    df = pd.read_csv('covid_19_india.csv')\n",
    "    df.Date = [datetime.strptime(x, '%d/%m/%y').strftime('%m-%d-%Y') for x in df.Date]\n",
    "    df = df.rename({'State/UnionTerritory': 'State', 'Cured': 'Recovered'}, axis=1)\n",
    "    df.drop([x for x in df.columns.values if x not in merged.columns.values], axis=1, inplace=True)\n",
    "    df['Country'] = 'India'\n",
    "    df['County'] = 'n/a'\n",
    "    return fillin(pd.concat([merged, df]))\n",
    "\n",
    "def deltas(merged):\n",
    "    def deltas(df):\n",
    "        for state in df['State'].unique():\n",
    "            for county in df[df['State'] == state]['County'].unique():\n",
    "                confirmed = df[(df['State'] == state) & (df['County'] == county)]['Confirmed'].values.tolist()\n",
    "                confirmed_deltas = [np.nan] + [confirmed[i] - confirmed[i-1] for i in range(1, len(confirmed))]\n",
    "                df.loc[(df['State'] == state) & (df['County'] == county), 'Confirmed_New'] = confirmed_deltas\n",
    "                deaths = df[(df['State'] == state) & (df['County'] == county)]['Deaths'].values.tolist()\n",
    "                deaths_deltas = [np.nan] + [deaths[i] - deaths[i-1] for i in range(1, len(deaths))]\n",
    "                df.loc[(df['State'] == state) & (df['County'] == county), 'Deaths_New'] = deaths_deltas\n",
    "                recovered = df[(df['State'] == state) & (df['County'] == county)]['Recovered'].values.tolist()\n",
    "                recovered_deltas = [np.nan] + [recovered[i] - recovered[i-1] for i in range(1, len(recovered))]\n",
    "                df.loc[(df['State'] == state) & (df['County'] == county), 'Recovered_New'] = recovered_deltas\n",
    "        return df\n",
    "\n",
    "    # Calculate deltas for each date\n",
    "    with ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        futures = [executor.submit(deltas, merged[(merged['Country'] == 'US') & (merged['State'] == state)].copy()) for state in merged[merged['Country'] == 'US'].State.unique()]\n",
    "        futures += [executor.submit(deltas, merged[merged['Country'] == country].copy()) for country in merged.Country.unique() if country != 'US']\n",
    "\n",
    "    final = pd.DataFrame(columns=merged.columns)\n",
    "    for future in as_completed(futures):\n",
    "        df = future.result()\n",
    "        final = pd.concat([final, df])\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n",
      "Some deltas are hugely negative!\n",
      "        Country    State      County        Date  Confirmed   Deaths  \\\n",
      "90685        UK  Unknown         n/a  07-02-2020        0.0      0.0   \n",
      "83891     Spain      n/a         n/a  04-30-2020   213435.0  24543.0   \n",
      "68160      Peru  Unknown         n/a  07-20-2020        0.0      0.0   \n",
      "29907   Ecuador      n/a         n/a  09-07-2020   110092.0  10576.0   \n",
      "6571     Brazil    Ceara         n/a  09-05-2020   222179.0   8565.0   \n",
      "...         ...      ...         ...         ...        ...      ...   \n",
      "7413     Brazil     Para         n/a  06-19-2020    80072.0   4469.0   \n",
      "91080        UK      n/a         n/a  04-13-2020    88621.0  11329.0   \n",
      "6425     Brazil    Bahia         n/a  08-05-2020   179737.0   3736.0   \n",
      "33630    France      n/a         n/a  08-26-2020   275640.0  30412.0   \n",
      "293748       US       MA  Unassigned  08-20-2020     9684.0      5.0   \n",
      "\n",
      "        Recovered  Confirmed_New  Deaths_New  Recovered_New  \n",
      "90685         0.0      -116237.0         0.0            0.0  \n",
      "83891    112050.0       -23464.0       268.0       -20879.0  \n",
      "68160    245081.0       -15839.0      -770.0         3126.0  \n",
      "29907     91242.0        -7953.0      3852.0       -11062.0  \n",
      "6571     195513.0        -7926.0        10.0          686.0  \n",
      "...           ...            ...         ...            ...  \n",
      "7413      66305.0         3449.0        74.0         -613.0  \n",
      "91080         0.0         4342.0       717.0         -344.0  \n",
      "6425     152838.0         4348.0        58.0        -6409.0  \n",
      "33630     73020.0         5185.0         0.0          -62.0  \n",
      "293748        0.0         6650.0      -126.0            0.0  \n",
      "\n",
      "[366 rows x 10 columns]\n",
      "Confirmed is much smaller than Deaths + Recovered!\n",
      "         Country              State County        Date  Confirmed  Deaths  \\\n",
      "3200   Australia  Western Australia    n/a  08-01-2020      641.0     9.0   \n",
      "7814      Brazil         Pernambuco    n/a  08-14-2020   110409.0  7111.0   \n",
      "7874      Brazil              Piaui    n/a  06-20-2020    12567.0   459.0   \n",
      "7879      Brazil              Piaui    n/a  06-25-2020    17080.0   546.0   \n",
      "7884      Brazil              Piaui    n/a  06-30-2020    20422.0   663.0   \n",
      "...          ...                ...    ...         ...        ...     ...   \n",
      "83691      Spain            Unknown    n/a  05-24-2020        0.0     0.0   \n",
      "83692      Spain            Unknown    n/a  05-25-2020        0.0     0.0   \n",
      "89402         UK    Channel Islands    n/a  05-31-2020      560.0    45.0   \n",
      "89403         UK    Channel Islands    n/a  06-01-2020      560.0    45.0   \n",
      "89404         UK    Channel Islands    n/a  06-02-2020      560.0    46.0   \n",
      "\n",
      "       Recovered  Confirmed_New  Deaths_New  Recovered_New  \n",
      "3200       652.0          -25.0         0.0            2.0  \n",
      "7814    175988.0         1567.0        27.0        87736.0  \n",
      "7874     13029.0            0.0        24.0        12295.0  \n",
      "7879     16570.0          853.0        12.0          895.0  \n",
      "7884     20479.0          669.0        20.0         1601.0  \n",
      "...          ...            ...         ...            ...  \n",
      "83691   150376.0            0.0         0.0            0.0  \n",
      "83692   150376.0            0.0         0.0            0.0  \n",
      "89402      528.0            0.0         0.0            3.0  \n",
      "89403      528.0            0.0         0.0            0.0  \n",
      "89404      528.0            0.0         1.0            0.0  \n",
      "\n",
      "[610 rows x 10 columns]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "merged = jhu()\n",
    "#merged = india(merged[merged['Country'] != 'India'])\n",
    "merged = deltas(merged)\n",
    "merged = merged.groupby(['Country', 'State', 'County', 'Date'], as_index=False).sum()\n",
    "\n",
    "# Write merged to CSV and verify\n",
    "merged.to_csv('jhu-daily-reports.csv', index=False)\n",
    "verify(merged)\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
